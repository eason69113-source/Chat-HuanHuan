# ğŸ’ƒ Chat-HuanHuan Â· å¬›å¬›é—®ç­”  
> æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äººâ€”â€”ç”„å¬›ï¼Œé™ªä½ ç•…èŠå®«å»·è¶£äº‹ã€‚
> ä¸€å¼  **â‰¤ 9 GB** æ˜¾å­˜çš„æ¶ˆè´¹å¡å°±èƒ½å®Œæˆ QLoRA å¾®è°ƒçš„ä¸­æ–‡å®«å»·è§’è‰²æ‰®æ¼”æ¨¡å‹ï¼  
> åŸºäº Meta-Llama-3.1-8B-Instruct + 4-bit é‡åŒ– + QLoRAï¼Œè®­ç»ƒä¸æ¨ç†å…¨ç¨‹æ˜¾å­˜å ç”¨ **< 9 GB**ï¼ŒRTX 3060/4060 å³å¯ç©è½¬ã€‚

---

âš”ï¸ æ˜¾å­˜å®æµ‹å¯¹æ¯”ï¼šä¸ºä»€ä¹ˆä½ çš„æ–¹æ¡ˆèƒ½è·‘é€šï¼Ÿ

| æ–¹æ¡ˆ | åŸºåº§æ¨¡å‹ | å¾®è°ƒæ–¹å¼ | é‡åŒ–ä½æ·± | è®­ç»ƒæ˜¾å­˜ (å®æµ‹) | æ¨ç†æ˜¾å­˜ (å®æµ‹) | ç¡¬ä»¶é—¨æ§› |
|------|----------|----------|----------|-----------------|-----------------|----------|
| å®˜æ–¹å…¨é‡å¾®è°ƒ | Llama-3-8B | Full Fine-tune | 16-bit | â‰ˆ 160 GB | â‰ˆ 16 GB | 2Ã— A100 (80G) |
| æ ‡å‡† LoRA | Llama-3-8B | LoRA r=8 | 16-bit | â‰ˆ 26â€“28 GB | â‰ˆ 16 GB | RTX 3090/4090 |
| **è¯¥é¡¹ç›®** | Llama-3-8B | LoRA r=8 | **4-bit NF4** | **â‰ˆ 7.5â€“8.5 GB** â­ | **â‰ˆ 5.5â€“6.5 GB** â­ | **RTX 3060 12G âœ…** |

---

## ğŸŒŸ æ•ˆæœé¢„è§ˆ
| çš‡ä¸Š | å¬›å¬› |
|------|------|
| çš‡ä¸Šï¼šä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½ã€‚ | å¬›å¬›ï¼šçš‡ä¸Šå¿§å›½å¿§æ°‘ï¼Œè‡£å¦¾æ„¿ä¸ºçš‡ä¸ŠæŠšç´ä¸€æ›²ï¼Œä»¥è§£çƒ¦å¿§ã€‚ |
| çš‡ä¸Šï¼šç»™æœ•è®²ä¸ªç¬‘è¯ã€‚ | å¬›å¬›ï¼šä»å‰æœ‰åº§å±±ï¼Œå±±é‡Œæœ‰åº§åº™ï¼Œåº™é‡Œæœ‰ä¸ªå°å¤ªç›‘åœ¨å·åƒæ¡‚èŠ±ç³•â€¦â€¦çš‡ä¸Šå¯è¿˜æ»¡æ„ï¼Ÿ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 0. åˆ›å»º conda è™šæ‹Ÿç¯å¢ƒï¼ˆPython 3.12ï¼‰
```bash
conda create -n huanhuan python=3.12 -y
conda activate huanhuan
```

### 1. å®‰è£… PyTorch 2.4.0 + CUDA 12.1ï¼ˆé˜¿é‡ŒæºåŠ é€Ÿï¼‰
```bash
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 \
    -f https://mirrors.aliyun.com/pytorch-wheels/cu121/
```

### 2. å…‹éš†ä»“åº“
```bash
git clone https://github.com/eason69113-source/Chat_HuanHuan.git
cd Chat-HuanHuan
```

### 3. ä¸‹è½½åŸºåº§æ¨¡å‹
```bash
python get_model.py   # è‡ªåŠ¨ä¸‹è½½ Llama-3.1-8B-Instruct åˆ° ./model
```

### 4. ä¸‹è½½ LoRA æƒé‡ï¼ˆå¯é€‰ï¼Œå·²è®­ç»ƒå¥½ï¼‰
```bash
# å·²éšä»“åº“é™„å¸¦ ./output/llama3_1_instruct_lora/checkpoint-700
# å¦‚æƒ³è‡ªå·±è®­ç»ƒï¼Œè§ã€Œè®­ç»ƒã€ç« èŠ‚
```

### 5. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 6. è¿è¡Œäº¤äº’å¼ demo
```bash
python inference.py
```
è¾“å…¥ `quit` é€€å‡ºã€‚

---

## ğŸ‹ï¸ è®­ç»ƒï¼ˆä»å¤´å¤ç°ï¼‰
1. å‡†å¤‡æ•°æ®  
   æŠŠå¯¹è¯ JSON æ”¾å…¥ `data/huanhuan.json`ï¼Œæ ¼å¼ç¤ºä¾‹ï¼š
   ```json
   [
     {"instruction": "çš‡ä¸Šï¼šä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ", "input": "", "output": "å¬›å¬›ï¼šå›çš‡ä¸Šï¼Œä»Šæ—¥é£å’Œæ—¥ä¸½ï¼Œå®œæ¸¸å›­èµèŠ±ã€‚"},
     ...
   ]
   ```
2. å¯åŠ¨è®­ç»ƒ  
   ```bash
   python train.py
   ```
   é»˜è®¤ 3 epochï¼Œå•å¡ A100(40G) çº¦ 2 å°æ—¶ã€‚  
   è®­ç»ƒå®Œè‡ªåŠ¨ä¿å­˜åˆ° `./output/llama3_1_instruct_lora/checkpoint-xxx`

---

## ğŸ“‚ é¡¹ç›®ç»“æ„
```
HuanHuan-Chat
â”œâ”€â”€ main
â”‚   â””â”€â”€ data/                     # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ get_model.py/             # ä¸‹è½½åŸºåº§æ¨¡å‹
â”‚   â””â”€â”€ train.py/                 # LoRA å¾®è°ƒè„šæœ¬
â”‚   â””â”€â”€ inference.py/             # äº¤äº’å¼æ¨ç†
â”œâ”€â”€ output
â”‚   â””â”€â”€ llama3_1_instruct_lora
â”‚       â””â”€â”€ checkpoint-700        # å·²è®­ç»ƒæƒé‡
â”œâ”€â”€ requirements.txt              # ç¯å¢ƒé…ç½®
â””â”€â”€ README.md
```

---

## âš™ï¸ ç¡¬ä»¶è¦æ±‚
| é˜¶æ®µ | æ˜¾å­˜ | å¤‡æ³¨ |
|------|------|------|
| æ¨ç† | â‰ˆ 6 GB | 4-bit é‡åŒ–ï¼Œå•å¡ 3060 å¯è·‘ |
| è®­ç»ƒ | â‰ˆ 9 GB | RTX 3060 12G |

---

## ğŸ› ï¸ æŠ€æœ¯æ ˆ
- **åŸºåº§æ¨¡å‹**: Meta-Llama-3.1-8B-Instruct  
- **å¾®è°ƒæ–¹æ¡ˆ**: LoRA (r=8, Î±=32, dropout=0.1)  
- **é‡åŒ–**: bitsandbytes 4-bit NF4  
- **æ¡†æ¶**: transformers + peft + modelscope  
- **å¯¹è¯æ¨¡æ¿**: Llama-3 å®˜æ–¹ chat_template

---

## ğŸ“„ å¼€æºåè®®
Apache-2.0  
âš ï¸ æ¨¡å‹æƒé‡è¯·éµå®ˆ Meta å®˜æ–¹[ç¤¾åŒºè®¸å¯](https://llama.meta.com/llama3/license/)ã€‚

---

## ğŸ™ è‡´è°¢
- Meta Llama-3 å›¢é˜Ÿ  
- ModelScope ç¤¾åŒº  
```
